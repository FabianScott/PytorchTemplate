{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A generic Neural Network to predict an output from an input\n",
    "    to be modified based on your specific use case.\n",
    "    For classification use an appropriate loss function such as\n",
    "    Cross-Entropy\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 inputSize,\n",
    "                 outputSize,\n",
    "                 nHiddenLayers=6,\n",
    "                 nNodes=128,\n",
    "                 lr=2e-4,\n",
    "                 dtype=torch.float32,\n",
    "                 activationFunction=nn.ReLU(),\n",
    "                 optimiser=torch.optim.Adam,\n",
    "                 lossFunction=nn.MSELoss(),\n",
    "                 constrainZeroOne=False,):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.outputSize = outputSize\n",
    "        self.mse_loss = lossFunction\n",
    "\n",
    "        for k in range(nHiddenLayers):\n",
    "            newSize = nNodes  # int(nNodes - nNodes * (k / (nHiddenLayers * 2))) for decreasing number of nodes per layer\n",
    "            self.layers.append(nn.Linear(inputSize, newSize, dtype=dtype))\n",
    "            self.layers.append(activationFunction)\n",
    "            inputSize = copy(newSize)\n",
    "\n",
    "        self.layers.append(nn.Linear(inputSize, outputSize, dtype=dtype))\n",
    "        if constrainZeroOne:\n",
    "            self.layers.append(nn.Sigmoid())    # To constrain output between (0, 1)\n",
    "\n",
    "        self.optimiser = optimiser(self.parameters(), lr=lr)    # I do not apologise for the British spelling of optimiser\n",
    "\n",
    "        # self.scheduler = ReduceLROnPlateau(self.optimiser, 'min') Look into scheduling if interested\n",
    "\n",
    "    # Define the forward function of the neural network\n",
    "    def forward(self, X):\n",
    "        x = X\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def trainBatch(self, X_batch, targets, epochs=1):\n",
    "        pbar = tqdm(range(epochs), desc=f'Training')\n",
    "        targets = targets.reshape((-1, self.outputSize))\n",
    "        losses = torch.zeros(epochs)\n",
    "\n",
    "        for i in pbar:\n",
    "            outputs = self.forward(X_batch)\n",
    "\n",
    "            loss = self.mse_loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.optimiser.step()\n",
    "            self.optimiser.zero_grad()\n",
    "            pbar.set_description(f'Training Error: {loss}')\n",
    "            losses[i] = loss\n",
    "            # self.scheduler.step(loss)\n",
    "        return losses\n",
    "\n",
    "        # return losses\n",
    "\n",
    "    def error(self, X_batch, targets):\n",
    "        outputs = self.forward(X_batch)\n",
    "        return self.mse_loss(outputs, targets.reshape((-1, 1)))\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.state_dict(), filename + '.pt')\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.load_state_dict(torch.load(filename + '.pt'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_device('cuda')\n",
    "    print('Using Cuda')\n",
    "else:\n",
    "    print('Cuda not available')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following 3 hyper-parameters will be used to distinguish different"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nNodes = 300\n",
    "nHiddenLayers = 4\n",
    "epochs = 1000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we create some dummy data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dtype = torch.float32\n",
    "\n",
    "X_train_numpy = np.random.standard_normal((100, 1))\n",
    "X_test_numpy = np.random.standard_normal((100, 1))\n",
    "y_train_numpy = np.random.standard_normal((100, 1))\n",
    "y_test_numpy = np.random.standard_normal((100, 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use this in the Neural Network it has to be put into tensors:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test = torch.tensor(X_train_numpy, dtype=dtype), torch.tensor(X_test_numpy, dtype=dtype)\n",
    "y_train, y_test = torch.tensor(y_train_numpy, dtype=dtype), torch.tensor(y_test_numpy, dtype=dtype)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now define the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = MLP(\n",
    "        inputSize=X_train.size(1),\n",
    "        outputSize=y_train.size(1),\n",
    "        nHiddenLayers=nHiddenLayers,\n",
    "        nNodes=nNodes,\n",
    "        optimiser=torch.optim.Adam,\n",
    "        activationFunction=nn.ReLU(),\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train using the trainBatch method:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network.trainBatch(X_train, y_train, epochs=epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To save the weights use the save method, here the hyperparameters are used in the filename to distinguish different runs:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network.save(f'NN_{nNodes}_{nHiddenLayers}_{epochs}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To load the network again use the load method, with the same filename"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network.load(f'Models/NN_N{nNodes}_{nHiddenLayers}_{epochs}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For analysis of your results, use the following to get the NN prediction on the test data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_NN = network.forward(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an example of what to plot, here the difference between the NN prediction and the true y-values across each element of the training data is plotted. Note how the network returns a tensor which must be detached in order for MatplotLib to use it:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_NN_plot = y_NN.cpu().detach().numpy()\n",
    "y_test_plot = y_test_numpy\n",
    "DiffSeries = y_test_plot - y_NN_plot\n",
    "ME = np.abs(DiffSeries).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the difference calculated, the following will plot it as a scatter plot."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alpha = .7  # Transparency of the points, so we can see overlapping points\n",
    "plt.scatter(range(len(DiffSeries)), DiffSeries,\n",
    "            label='Error', alpha=alpha, s=.7,\n",
    "            facecolors='none', edgecolors='b', )\n",
    "plt.title(f'Error between Dref and NN demand over time ME={ME}')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'Plots/DifferencePlot_{nNodes}_{nHiddenLayers}_{epochs}.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To save the figure, uncoment the plt.savefig line"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% m\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
